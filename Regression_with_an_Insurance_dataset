{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example,running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s4e12/train.csv\")\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.describe())\nprint(train_df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_data = pd.DataFrame()\nnull_data['null_count'] = train_df.isnull().sum()\nnull_data['null_ratio'] = round(null_data['null_count'] / len(train_df), 4)\nnull_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_df = train_df.dropna()\nprint(train_df[\"Number of Dependents\"].describe().astype(int))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\ncategorical_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_null_data = pd.DataFrame()\ncategorical_null_data['null_count'] = train_df[categorical_cols].isnull().sum()\ncategorical_null_data['null_ratio'] = round(categorical_null_data['null_count'] / len(train_df), 4)\ncategorical_null_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for c in categorical_cols:\n    print(f'Category: {c}, Count: {len(train_df[c].unique())} Unique values: {train_df[c].unique()}')\n\nfor c in categorical_cols:\n    print(f'Category: {c}')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Category: Gender ////\n# Category: Marital Status ////\n# Category: Education Level /\n# Category: Occupation ////\n# Category: Location //\n# Category: Policy Type ///\n# Category: Policy Start Date ///\n# Category: Customer Feedback /\n# Category: Smoking Status /\n# Category: Exercise Frequency /\n# Category: Property Type //\n\nimport pandas as pd\n\ndf = train_df.copy()\n\nbinary_mappings = {\n    'Smoking Status': {'No': 0, 'Yes': 1}\n}\nfor col, mapping in binary_mappings.items():\n    if col in df.columns:\n        df[col] = df[col].map(mapping)\n\nonehot_encode_cols = ['Gender', 'Location', 'Policy Type', 'Property Type']\ndf = pd.get_dummies(df, columns=onehot_encode_cols, drop_first=False)\n\nordinal_mappings = {\n    'Exercise Frequency': {'Rarely': 0, 'Monthly': 1, 'Weekly': 2, 'Daily': 3},\n    'Customer Feedback': {'Poor': 0, 'Average': 1, 'Good': 2},\n    'Education Level': {'High School': 0, \"Bachelor's\": 1, \"Master's\": 2, 'PhD': 3},\n    'Policy Type': {'Premium': 2, 'Comprehensive': 1, 'Basic': 0}\n}\nfor col, mapping in ordinal_mappings.items():\n    if col in df.columns:\n        df[col] = df[col].map(mapping)\n\nif 'Policy Start Date' in df.columns:\n    df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date']).astype(int) / 10**9  \n\ncategorical_cols = ['Customer Feedback', 'Marital Status', 'Occupation']\nfor col in categorical_cols:\n    df[col] = df[col].fillna('Unknown')\n\nlabel_encode_cols = ['Customer Feedback', 'Marital Status', 'Occupation']\nfor col in label_encode_cols:\n    df[col] = df[col].astype('category').cat.codes\n\nprint(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(df)\ndf.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfig, axes = plt.subplots(len(categorical_cols) + 1, 1, figsize=(10, 5 * (len(categorical_cols) + 1)))\nfig.tight_layout(pad=6.0)\n\nfor i, col in enumerate(categorical_cols):\n    sns.boxplot(x=train_df[col], y=train_df['Premium Amount'], ax=axes[i])\n    axes[i].set_title(f'Boxplot of {col} vs Premium Amount')\n    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)\n\ncorr_matrix = train_df.select_dtypes(include=['number']).corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[len(categorical_cols)])\naxes[len(categorical_cols)].set_title('Correlation Matrix Heatmap')\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# train_df = train_df.drop(columns=categorical_cols)\n# train_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\ndf = df.fillna(df.mean())\n\ndf['Mean_Income_Duration'] = (df['Annual Income'] + df['Insurance Duration']) / 2\ndf['Income_Skewness'] = skew(df['Annual Income'], nan_policy='omit')\ndf['Health_Skewness'] = skew(df['Health Score'], nan_policy='omit')\ndf['Claims_Skewness'] = skew(df['Previous Claims'], nan_policy='omit')\n\ndf['Income_Kurtosis'] = kurtosis(df['Annual Income'], nan_policy='omit')\ndf['Health_Kurtosis'] = kurtosis(df['Health Score'], nan_policy='omit')\ndf['Claims_Kurtosis'] = kurtosis(df['Previous Claims'], nan_policy='omit')\n\ndf['Income_Per_Dependent'] = df['Annual Income'] / (df['Number of Dependents'] + 1)  \ndf['Claims_Per_Year'] = df['Previous Claims'] / df['Insurance Duration']\n\ndf['Age_to_VehicleAge_Diff'] = df['Age'] - df['Vehicle Age']\ndf['Health_to_Credit_Score_Ratio'] = df['Health Score'] / (df['Credit Score'] + 1)\n\ndf['Total_Assets'] = df['Annual Income'] + df['Credit Score']\ndf['Overall_Risk'] = (df['Vehicle Age'] + df['Previous Claims'] + df['Insurance Duration']) / df['Health Score']\n\ndf['Log_Income'] = np.log1p(df['Annual Income'])\ndf['Log_Credit'] = np.log1p(df['Credit Score'])\n\n\ndf['Income_Quartile'] = pd.qcut(df['Annual Income'], 4, labels=[1, 2, 3, 4])\ndf['Credit_Quartile'] = pd.qcut(df['Credit Score'], 4, labels=[1, 2, 3, 4])\n\ndf['High_Income'] = (df['Annual Income'] > df['Annual Income'].median()).astype(int)\ndf['Old_Vehicle'] = (df['Vehicle Age'] > 10).astype(int)\n\n# Display the enriched dataset\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_data = pd.DataFrame()\nnull_data['null_count'] = df.isnull().sum()\nnull_data['null_ratio'] = round(null_data['null_count'] / len(df), 4)\nnull_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_log_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Separate features and target\nX = df.drop(columns=['Premium Amount'])\ny = df['Premium Amount']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize features (important for NN models)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Log-transform the target variable to stabilize variance\ny_train = np.log1p(y_train)\ny_test = np.log1p(y_test)\n\n# Build the Neural Network model\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.1),\n    Dense(32, activation='relu'),\n    Dropout(0.05),\n    Dense(16, activation='relu'),\n    Dense(1, activation='linear')  # Output layer for regression\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n\n# Train the model\nhistory = model.fit(X, y, validation_split=0.2, epochs=25, batch_size=32, verbose=1)\n\n# Predict and Evaluate\ny_pred = model.predict(X_test)\ny_pred = np.expm1(y_pred)  # Reverse the log transformation\ny_test = np.expm1(y_test)  # Reverse the log transformation\n\n# Calculate RMSLE\nrmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n\nprint(f\"Root Mean Squared Logarithmic Error (RMSLE): {rmsle}\")\n\n# Predict method\ndef predict_premium(X_input):\n    X_input_scaled = scaler.transform(X_input)\n    predictions = model.predict(X_input_scaled)\n    return np.expm1(predictions)  # Reverse the log transformation\n\n# Example usage of predict\nexample_data = X_test[:5]  # Replace with new input data\npredictions = predict_premium(example_data)\nprint(\"Predicted Premium Amounts:\", predictions)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"add_compound_data(convert_categorical_to_numeric(test_df), scaler = scaler, fit_scaler=False).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/playground-series-s4e12/test.csv\")\n\ntest_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_categorical_to_numeric(df):\n    binary_mappings = {\n        'Smoking Status': {'No': 0, 'Yes': 1}\n    }\n    for col, mapping in binary_mappings.items():\n        if col in df.columns:\n            df[col] = df[col].map(mapping)\n    \n    onehot_encode_cols = ['Gender', 'Location', 'Policy Type', 'Property Type']\n    df = pd.get_dummies(df, columns=onehot_encode_cols, drop_first=False)\n    \n    ordinal_mappings = {\n        'Exercise Frequency': {'Rarely': 0, 'Monthly': 1, 'Weekly': 2, 'Daily': 3},\n        'Customer Feedback': {'Poor': 0, 'Average': 1, 'Good': 2},\n        'Education Level': {'High School': 0, \"Bachelor's\": 1, \"Master's\": 2, 'PhD': 3},\n        'Policy Type': {'Premium': 2, 'Comprehensive': 1, 'Basic': 0}\n    }\n    for col, mapping in ordinal_mappings.items():\n        if col in df.columns:\n            df[col] = df[col].map(mapping)\n    \n    if 'Policy Start Date' in df.columns:\n        df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date']).astype(int) / 10**9  \n    \n    categorical_cols = ['Customer Feedback', 'Marital Status', 'Occupation']\n    for col in categorical_cols:\n        df[col] = df[col].fillna('Unknown')\n    \n    label_encode_cols = ['Customer Feedback', 'Marital Status', 'Occupation']\n    for col in label_encode_cols:\n        df[col] = df[col].astype('category').cat.codes\n    \n    return df\nconvert_categorical_to_numeric(test_df).shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_compound_data(df, scaler=None, fit_scaler=False):\n    df = df.fillna(df.mean())\n    df[\"Smoking Status\"] = 0.5\n\n    # Add new features\n    df['Mean_Income_Duration'] = (df['Annual Income'] + df['Insurance Duration']) / 2\n    df['Income_Skewness'] = skew(df['Annual Income'], nan_policy='omit')\n    df['Health_Skewness'] = skew(df['Health Score'], nan_policy='omit')\n    df['Claims_Skewness'] = skew(df['Previous Claims'], nan_policy='omit')\n\n    df['Income_Kurtosis'] = kurtosis(df['Annual Income'], nan_policy='omit')\n    df['Health_Kurtosis'] = kurtosis(df['Health Score'], nan_policy='omit')\n    df['Claims_Kurtosis'] = kurtosis(df['Previous Claims'], nan_policy='omit')\n\n    df['Income_Per_Dependent'] = df['Annual Income'] / (df['Number of Dependents'] + 1)\n    df['Claims_Per_Year'] = df['Previous Claims'] / df['Insurance Duration']\n    df['Age_to_VehicleAge_Diff'] = df['Age'] - df['Vehicle Age']\n    df['Health_to_Credit_Score_Ratio'] = df['Health Score'] / (df['Credit Score'] + 1)\n\n    df['Total_Assets'] = df['Annual Income'] + df['Credit Score']\n    df['Overall_Risk'] = (df['Vehicle Age'] + df['Previous Claims'] + df['Insurance Duration']) / df['Health Score']\n\n    # Avoid issues with log transformation\n    df['Log_Income'] = np.log1p(df['Annual Income'])\n    df['Log_Credit'] = np.log1p(df['Credit Score'])\n\n    df['Income_Quartile'] = pd.qcut(df['Annual Income'], 4, labels=[1, 2, 3, 4]).astype(int)\n    df['Credit_Quartile'] = pd.qcut(df['Credit Score'], 4, labels=[1, 2, 3, 4]).astype(int)\n\n    df['High_Income'] = (df['Annual Income'] > df['Annual Income'].median()).astype(int)\n    df['Old_Vehicle'] = (df['Vehicle Age'] > 10).astype(int)\n\n\n\n    df = scaler.transform(df)\n\n    return df\n\n\ntransformed_test_df = convert_categorical_to_numeric(test_df)\ntransformed_test_df = add_compound_data(transformed_test_df, scaler=scaler, fit_scaler=False)\n# transformed_test_df = transformed_test_df.apply(pd.to_numeric, errors='coerce')\ntransformed_test_df = scaler.transform(transformed_test_df)\ntransformed_test_df = pd.DataFrame(transformed_test_df, columns=X.columns)\ntransformed_test_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any NaN values:\", np.any(np.isnan(transformed_test_df)))\nprint(\"Any Inf values:\", np.any(np.isinf(transformed_test_df)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_data = pd.DataFrame()\nnull_data['null_count'] = test_df.isnull().sum()\nnull_data['null_ratio'] = round(null_data['null_count'] / len(test_df), 4)\nnull_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"transformed_test_df shape:\", transformed_test_df.shape)\nprint(\"transformed_test_df dtype:\", transformed_test_df.dtype)\nprint(\"example_data shape:\", example_data.shape)\nprint(\"example_data dtype:\", example_data.dtype)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(\"Columns in transformed_test_df:\", transformed_test_df.columns)\nprint(\"Columns in X_train:\", X.columns)\nmissing_columns = set(X.columns) - set(transformed_test_df.columns)\nextra_columns = set(transformed_test_df.columns) - set(X.columns)\nprint(\"Missing columns:\", missing_columns)\nprint(\"Extra columns:\", extra_columns)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Min value in transformed_test_df:\", transformed_test_df.min())\nprint(\"Max value in transformed_test_df:\", transformed_test_df.max())\nprint(\"Mean value in transformed_test_df:\", transformed_test_df.mean())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transformed_test_df = np.clip(transformed_test_df, -10, 10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.zeros((20, 46), dtype='float32')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16\npredictions = []\n\nfor i in range(0, transformed_test_df.shape[0], batch_size):\n    batch = transformed_test_df[i:i + batch_size]\n    batch_predictions = model.predict(batch)\n    predictions.append(batch_predictions)\n\n# Concatenate predictions\npredictions = np.concatenate(predictions, axis=0)\nprint(\"Final predictions shape:\", predictions.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Transformed test data shape:\", transformed_test_df.shape)\nprint(\"Transformed test data dtype:\", transformed_test_df.dtype)\nprint(\"Model input shape:\", model.input_shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any NaN in transformed_test_df:\", np.any(np.isnan(transformed_test_df)))\nprint(\"Any Inf in transformed_test_df:\", np.any(np.isinf(transformed_test_df)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.backend.clear_session()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(example_data[0])\n# predict_premium(example_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\npredictions = []\n\nfor i in range(0, transformed_test_df.shape[0], batch_size):\n    # Extract batch\n    batch = transformed_test_df[i:i + batch_size]\n    \n    # Ensure batch is a 2D array of dtype float32\n    batch = batch.astype('float32')\n    \n    # Check batch shape and dtype\n    print(f\"Processing batch {i // batch_size + 1}, shape: {batch.shape}, dtype: {batch.dtype}\")\n    \n    # Predict on the batch\n    batch_predictions = model.predict(batch)\n    predictions.append(batch_predictions)\n\n# Concatenate all batch predictions\npredictions = np.concatenate(predictions, axis=0)\nprint(\"Final predictions shape:\", predictions.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_data = pd.DataFrame()\nnull_data['null_count'] = transformed_test_df.isnull().sum()\nnull_data['null_ratio'] = round(null_data['null_count'] / len(transformed_test_df), 4)\nnull_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_premium(transformed_test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = test_df[['id']]\nsubmission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_test_df = transformed_test_df[X.columns]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission[\"Premium Amount\"] = predict_premium(transformed_test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shape of training data:\", X.shape)\nprint(\"Shape of test data:\", transformed_test_df.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nsubmission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}