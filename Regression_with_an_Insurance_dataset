{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:29.808109Z","iopub.execute_input":"2024-12-07T13:43:29.808518Z","iopub.status.idle":"2024-12-07T13:43:30.933249Z","shell.execute_reply.started":"2024-12-07T13:43:29.808482Z","shell.execute_reply":"2024-12-07T13:43:30.931905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s4e12/train.csv\")\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:30.935638Z","iopub.execute_input":"2024-12-07T13:43:30.936198Z","iopub.status.idle":"2024-12-07T13:43:37.296886Z","shell.execute_reply.started":"2024-12-07T13:43:30.936151Z","shell.execute_reply":"2024-12-07T13:43:37.295782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.298393Z","iopub.execute_input":"2024-12-07T13:43:37.298836Z","iopub.status.idle":"2024-12-07T13:43:37.340471Z","shell.execute_reply.started":"2024-12-07T13:43:37.298792Z","shell.execute_reply":"2024-12-07T13:43:37.339483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.341925Z","iopub.execute_input":"2024-12-07T13:43:37.342346Z","iopub.status.idle":"2024-12-07T13:43:37.376336Z","shell.execute_reply.started":"2024-12-07T13:43:37.342299Z","shell.execute_reply":"2024-12-07T13:43:37.375339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_data = pd.DataFrame()\nnull_data['null_count'] = train_df.isnull().sum()\nnull_data['null_ratio'] = null_data['null_count'] / len(train_df)\nnull_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.378637Z","iopub.execute_input":"2024-12-07T13:43:37.378937Z","iopub.status.idle":"2024-12-07T13:43:37.397766Z","shell.execute_reply.started":"2024-12-07T13:43:37.378908Z","shell.execute_reply":"2024-12-07T13:43:37.396741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.398947Z","iopub.execute_input":"2024-12-07T13:43:37.399203Z","iopub.status.idle":"2024-12-07T13:43:37.418698Z","shell.execute_reply.started":"2024-12-07T13:43:37.399177Z","shell.execute_reply":"2024-12-07T13:43:37.417860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"null_data = pd.DataFrame()\nnull_data['null_count'] = train_df.isnull().sum()\nnull_data['null_ratio'] = null_data['null_count'] / len(train_df)\nnull_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.419879Z","iopub.execute_input":"2024-12-07T13:43:37.420170Z","iopub.status.idle":"2024-12-07T13:43:37.440972Z","shell.execute_reply.started":"2024-12-07T13:43:37.420142Z","shell.execute_reply":"2024-12-07T13:43:37.439894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.442264Z","iopub.execute_input":"2024-12-07T13:43:37.442605Z","iopub.status.idle":"2024-12-07T13:43:37.451281Z","shell.execute_reply.started":"2024-12-07T13:43:37.442570Z","shell.execute_reply":"2024-12-07T13:43:37.450236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\ncategorical_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.452583Z","iopub.execute_input":"2024-12-07T13:43:37.452988Z","iopub.status.idle":"2024-12-07T13:43:37.466276Z","shell.execute_reply.started":"2024-12-07T13:43:37.452946Z","shell.execute_reply":"2024-12-07T13:43:37.465323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pd.get_dummies(train_df, columns=categorical_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.467302Z","iopub.execute_input":"2024-12-07T13:43:37.467629Z","iopub.status.idle":"2024-12-07T13:43:37.475516Z","shell.execute_reply.started":"2024-12-07T13:43:37.467600Z","shell.execute_reply":"2024-12-07T13:43:37.474340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfig, axes = plt.subplots(len(categorical_cols) + 1, 1, figsize=(10, 5 * (len(categorical_cols) + 1)))\nfig.tight_layout(pad=6.0)\n\nfor i, col in enumerate(categorical_cols):\n    sns.boxplot(x=train_df[col], y=train_df['Premium Amount'], ax=axes[i])\n    axes[i].set_title(f'Boxplot of {col} vs Premium Amount')\n    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)\n\ncorr_matrix = train_df.select_dtypes(include=['number']).corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[len(categorical_cols)])\naxes[len(categorical_cols)].set_title('Correlation Matrix Heatmap')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:43:37.476984Z","iopub.execute_input":"2024-12-07T13:43:37.477733Z","iopub.status.idle":"2024-12-07T13:44:35.440972Z","shell.execute_reply.started":"2024-12-07T13:43:37.477679Z","shell.execute_reply":"2024-12-07T13:44:35.439793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Drop categorical variables because they are quite useless","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(columns=categorical_cols)\ntrain_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:49:21.561651Z","iopub.execute_input":"2024-12-07T13:49:21.562137Z","iopub.status.idle":"2024-12-07T13:49:21.588115Z","shell.execute_reply.started":"2024-12-07T13:49:21.562089Z","shell.execute_reply":"2024-12-07T13:49:21.587032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\n# Load the dataset\n# Assuming `df` is your DataFrame\ndf = train_df.copy()\ndf = df.fillna(df.mean())\n\ndf['Mean_Income_Duration'] = (df['Annual Income'] + df['Insurance Duration']) / 2\ndf['Income_Skewness'] = skew(df['Annual Income'], nan_policy='omit')\ndf['Health_Skewness'] = skew(df['Health Score'], nan_policy='omit')\ndf['Claims_Skewness'] = skew(df['Previous Claims'], nan_policy='omit')\n\ndf['Income_Kurtosis'] = kurtosis(df['Annual Income'], nan_policy='omit')\ndf['Health_Kurtosis'] = kurtosis(df['Health Score'], nan_policy='omit')\ndf['Claims_Kurtosis'] = kurtosis(df['Previous Claims'], nan_policy='omit')\n\n# 2. Interaction Features\ndf['Income_Per_Dependent'] = df['Annual Income'] / (df['Number of Dependents'] + 1)  # Avoid division by zero\ndf['Claims_Per_Year'] = df['Previous Claims'] / df['Insurance Duration']\n\n# 3. Ratios and Differences\ndf['Age_to_VehicleAge_Diff'] = df['Age'] - df['Vehicle Age']\ndf['Health_to_Credit_Score_Ratio'] = df['Health Score'] / (df['Credit Score'] + 1)\n\n# 4. Aggregated Features\ndf['Total_Assets'] = df['Annual Income'] + df['Credit Score']\ndf['Overall_Risk'] = (df['Vehicle Age'] + df['Previous Claims'] + df['Insurance Duration']) / df['Health Score']\n\n# 5. Log Transformations\ndf['Log_Income'] = np.log1p(df['Annual Income'])\ndf['Log_Credit'] = np.log1p(df['Credit Score'])\n\n\n# 7. Percentiles (Quartiles)\ndf['Income_Quartile'] = pd.qcut(df['Annual Income'], 4, labels=[1, 2, 3, 4])\ndf['Credit_Quartile'] = pd.qcut(df['Credit Score'], 4, labels=[1, 2, 3, 4])\n\n# 8. Categorical Feature Engineering (if applicable)\ndf['High_Income'] = (df['Annual Income'] > df['Annual Income'].median()).astype(int)\ndf['Old_Vehicle'] = (df['Vehicle Age'] > 10).astype(int)\n\n# Display the enriched dataset\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:04:55.739754Z","iopub.execute_input":"2024-12-07T14:04:55.741643Z","iopub.status.idle":"2024-12-07T14:04:55.799823Z","shell.execute_reply.started":"2024-12-07T14:04:55.741572Z","shell.execute_reply":"2024-12-07T14:04:55.798783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:04:58.173815Z","iopub.execute_input":"2024-12-07T14:04:58.174231Z","iopub.status.idle":"2024-12-07T14:04:58.181256Z","shell.execute_reply.started":"2024-12-07T14:04:58.174197Z","shell.execute_reply":"2024-12-07T14:04:58.180174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_log_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Separate features and target\nX = df.drop(columns=['Premium Amount'])\ny = df['Premium Amount']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize features (important for NN models)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Log-transform the target variable to stabilize variance\ny_train = np.log1p(y_train)\ny_test = np.log1p(y_test)\n\n# Build the Neural Network model\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.1),\n    Dense(32, activation='relu'),\n    Dropout(0.05),\n    Dense(16, activation='relu'),\n    Dense(1, activation='linear')  # Output layer for regression\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=32, verbose=1)\n\n# Predict and Evaluate\ny_pred = model.predict(X_test)\ny_pred = np.expm1(y_pred)  # Reverse the log transformation\ny_test = np.expm1(y_test)  # Reverse the log transformation\n\n# Calculate RMSLE\nrmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n\nprint(f\"Root Mean Squared Logarithmic Error (RMSLE): {rmsle}\")\n\n# Predict method\ndef predict_premium(X_input):\n    X_input_scaled = scaler.transform(X_input)\n    predictions = model.predict(X_input_scaled)\n    return np.expm1(predictions)  # Reverse the log transformation\n\n# Example usage of predict\nexample_data = X_test[:5]  # Replace with new input data\npredictions = predict_premium(example_data)\nprint(\"Predicted Premium Amounts:\", predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:22:48.015652Z","iopub.execute_input":"2024-12-07T14:22:48.016553Z","iopub.status.idle":"2024-12-07T14:23:06.298481Z","shell.execute_reply.started":"2024-12-07T14:22:48.016479Z","shell.execute_reply":"2024-12-07T14:23:06.297471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/playground-series-s4e12/test.csv\")\n\ntest_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:33:09.081261Z","iopub.execute_input":"2024-12-07T14:33:09.081729Z","iopub.status.idle":"2024-12-07T14:33:12.341222Z","shell.execute_reply.started":"2024-12-07T14:33:09.081680Z","shell.execute_reply":"2024-12-07T14:33:12.339943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NaN values only in numeric columns\nnumeric_cols = test_df.select_dtypes(include=[np.number]).columns  # Select numeric columns\ntest_df[numeric_cols] = test_df[numeric_cols].fillna(test_df[numeric_cols].mean())\n\n# Verify that there are no NaN values in numeric columns\nprint(test_df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:36:24.075266Z","iopub.execute_input":"2024-12-07T14:36:24.075676Z","iopub.status.idle":"2024-12-07T14:38:27.974909Z","shell.execute_reply.started":"2024-12-07T14:36:24.075642Z","shell.execute_reply":"2024-12-07T14:38:27.973038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform_data(df, scaler=None, fit_scaler=False):\n    df = df.drop(columns=categorical_cols, errors='ignore')  # Drop categorical columns if present\n    df = df.fillna(df.mean())\n\n    # Add new features\n    df['Mean_Income_Duration'] = (df['Annual Income'] + df['Insurance Duration']) / 2\n    df['Income_Skewness'] = skew(df['Annual Income'], nan_policy='omit')\n    df['Health_Skewness'] = skew(df['Health Score'], nan_policy='omit')\n    df['Claims_Skewness'] = skew(df['Previous Claims'], nan_policy='omit')\n\n    df['Income_Kurtosis'] = kurtosis(df['Annual Income'], nan_policy='omit')\n    df['Health_Kurtosis'] = kurtosis(df['Health Score'], nan_policy='omit')\n    df['Claims_Kurtosis'] = kurtosis(df['Previous Claims'], nan_policy='omit')\n\n    df['Income_Per_Dependent'] = df['Annual Income'] / (df['Number of Dependents'] + 1)\n    df['Claims_Per_Year'] = df['Previous Claims'] / df['Insurance Duration']\n    df['Age_to_VehicleAge_Diff'] = df['Age'] - df['Vehicle Age']\n    df['Health_to_Credit_Score_Ratio'] = df['Health Score'] / (df['Credit Score'] + 1)\n\n    df['Total_Assets'] = df['Annual Income'] + df['Credit Score']\n    df['Overall_Risk'] = (df['Vehicle Age'] + df['Previous Claims'] + df['Insurance Duration']) / df['Health Score']\n\n    # Avoid issues with log transformation\n    df['Log_Income'] = np.log1p(df['Annual Income'])\n    df['Log_Credit'] = np.log1p(df['Credit Score'])\n\n    df['Income_Quartile'] = pd.qcut(df['Annual Income'], 4, labels=[1, 2, 3, 4]).astype(int)\n    df['Credit_Quartile'] = pd.qcut(df['Credit Score'], 4, labels=[1, 2, 3, 4]).astype(int)\n\n    df['High_Income'] = (df['Annual Income'] > df['Annual Income'].median()).astype(int)\n    df['Old_Vehicle'] = (df['Vehicle Age'] > 10).astype(int)\n\n\n\n    # Scale the data\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    if scaler is not None:\n        if fit_scaler:\n            scaler.fit(df[numeric_cols])\n        df[numeric_cols] = scaler.transform(df[numeric_cols])\n\n    return df\n\ntransformed_test_df = transform_data(test_df, scaler=scaler, fit_scaler=False)\ntransformed_test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:41:53.351050Z","iopub.execute_input":"2024-12-07T14:41:53.351464Z","iopub.status.idle":"2024-12-07T14:41:54.901231Z","shell.execute_reply.started":"2024-12-07T14:41:53.351428Z","shell.execute_reply":"2024-12-07T14:41:54.900115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:42:00.167392Z","iopub.execute_input":"2024-12-07T14:42:00.167851Z","iopub.status.idle":"2024-12-07T14:42:00.175045Z","shell.execute_reply.started":"2024-12-07T14:42:00.167809Z","shell.execute_reply":"2024-12-07T14:42:00.173914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(test_df.drop(columns=categorical_cols, errors='ignore').dtypes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:40:29.913483Z","iopub.execute_input":"2024-12-07T14:40:29.913936Z","iopub.status.idle":"2024-12-07T14:40:29.955700Z","shell.execute_reply.started":"2024-12-07T14:40:29.913896Z","shell.execute_reply":"2024-12-07T14:40:29.954606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformed_test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:24:48.908086Z","iopub.execute_input":"2024-12-07T14:24:48.908472Z","iopub.status.idle":"2024-12-07T14:24:48.915576Z","shell.execute_reply.started":"2024-12-07T14:24:48.908438Z","shell.execute_reply":"2024-12-07T14:24:48.914442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:25:28.416549Z","iopub.execute_input":"2024-12-07T14:25:28.416949Z","iopub.status.idle":"2024-12-07T14:25:28.423259Z","shell.execute_reply.started":"2024-12-07T14:25:28.416915Z","shell.execute_reply":"2024-12-07T14:25:28.422301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_premium(transformed_test_df).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:42:05.872563Z","iopub.execute_input":"2024-12-07T14:42:05.872967Z","iopub.status.idle":"2024-12-07T14:42:44.564858Z","shell.execute_reply.started":"2024-12-07T14:42:05.872932Z","shell.execute_reply":"2024-12-07T14:42:44.563620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = test_df[['id']]\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:42:48.817439Z","iopub.execute_input":"2024-12-07T14:42:48.817913Z","iopub.status.idle":"2024-12-07T14:42:48.830893Z","shell.execute_reply.started":"2024-12-07T14:42:48.817877Z","shell.execute_reply":"2024-12-07T14:42:48.829717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission[\"Premium Amount\"] = predict_premium(transformed_test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:42:52.115782Z","iopub.execute_input":"2024-12-07T14:42:52.116184Z","iopub.status.idle":"2024-12-07T14:43:26.970499Z","shell.execute_reply.started":"2024-12-07T14:42:52.116148Z","shell.execute_reply":"2024-12-07T14:43:26.969398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:43:36.889630Z","iopub.execute_input":"2024-12-07T14:43:36.890055Z","iopub.status.idle":"2024-12-07T14:43:36.903056Z","shell.execute_reply.started":"2024-12-07T14:43:36.890020Z","shell.execute_reply":"2024-12-07T14:43:36.901912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:43:45.918519Z","iopub.execute_input":"2024-12-07T14:43:45.918964Z","iopub.status.idle":"2024-12-07T14:43:47.197706Z","shell.execute_reply.started":"2024-12-07T14:43:45.918926Z","shell.execute_reply":"2024-12-07T14:43:47.196776Z"}},"outputs":[],"execution_count":null}]}